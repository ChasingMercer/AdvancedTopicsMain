{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee1f475-bf30-4384-bfe5-e93947c166c2",
   "metadata": {
    "id": "lVVgRws2wgT0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import random_split \n",
    "from transformers import BeitImageProcessor, BeitForImageClassification, Trainer, AutoFeatureExtractor, TrainingArguments\n",
    "from torch.utils.data import TensorDataset\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import evaluate\n",
    "import huggingface_hub\n",
    "from transformers import AutoFeatureExtractor\n",
    "from evaluate import evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5667fbde-b25f-4ec9-882e-01b6176353f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac6cfeb9a1a446886b2b68bae66912d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75afcc6-0e7e-47c4-bd31-db5cef23e87f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Initialise Cuda and check that Cuda is available\n",
    "device = torch.device(\"cuda\")\n",
    "print(device)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc58c2e6-45f9-473b-8edb-c1bb132836f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felixmorgan/miniconda3/envs/pythonProject/lib/python3.9/site-packages/transformers/models/beit/feature_extraction_beit.py:28: FutureWarning: The class BeitFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use BeitImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "processor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n",
    "model = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0ceebd-441f-45be-a2fe-01ce76c79590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'height': 224, 'width': 224}\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n",
    "\n",
    "print(feature_extractor.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7438c2b8-a734-4832-89ae-9cd8d55e146e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a0f38c1f9a4c4faff09bf38834c56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (/home/felixmorgan/.cache/huggingface/datasets/imagefolder/dataset-69da99f399a4f097/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4b08884b534516bd8a0faeb9fd2d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load Dataset\n",
    "ds = load_dataset(\"./dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f9a4824-266d-4282-ad0f-4d15d73cb99d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/felixmorgan/.cache/huggingface/datasets/imagefolder/dataset-69da99f399a4f097/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f/cache-714093f88edeae8e.arrow\n"
     ]
    }
   ],
   "source": [
    "shuffled = ds.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47f25e50-9bb3-4cee-a1f4-96e0e740bb1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8     1160\n",
      "1      851\n",
      "0      698\n",
      "9      692\n",
      "3      639\n",
      "10     621\n",
      "4      591\n",
      "6      526\n",
      "2      475\n",
      "5      377\n",
      "7      232\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "labels = pd.Series(shuffled['train']['label'])\n",
    "\n",
    "print(labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38b51c28-d338-4616-a0a1-095df659cbd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = list(labels.value_counts().loc[lambda x : x>90].keys())\n",
    "include_index = [i for i, j in enumerate(labels) if j in x]\n",
    "include = [j for i, j in enumerate(labels) if j in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08acdd5d-f136-4a94-8f57-5d691a019b89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n"
     ]
    }
   ],
   "source": [
    "print(len(set(x)))\n",
    "print(set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f6a4ad5-0d1b-4de0-9098-4918c3c00d66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 500\n",
      "10 500\n",
      "6 500\n",
      "4 500\n",
      "3 500\n",
      "8 500\n",
      "2 474\n",
      "0 500\n",
      "5 376\n",
      "1 500\n",
      "7 231\n",
      "5081\n",
      "[29, 30, 31, 54, 88, 93, 105, 106, 107, 113]\n"
     ]
    }
   ],
   "source": [
    "def select_indexes(lst, lst_index):\n",
    "    idx_dict = {}\n",
    "    for i, val in enumerate(lst):\n",
    "        if val not in idx_dict:\n",
    "            idx_dict[val] = []\n",
    "        elif len(idx_dict[val]) < 500:\n",
    "            idx_dict[val].append(lst_index[i])\n",
    "        elif all(len(v) == 500 for v in idx_dict.values()):\n",
    "            break\n",
    "    return idx_dict\n",
    "\n",
    "sample = []\n",
    "for key, value in select_indexes(include, include_index).items():\n",
    "    print(key, len(value))\n",
    "    sample += value\n",
    "\n",
    "print(len(sample))\n",
    "print(sample[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "22c9ec43-59e6-4baa-93d7-21f4b82746b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampled = shuffled['train'].select(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4aeddc22-0a9d-41df-b152-bb74748e532a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop([224, 224]),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize([224, 224]),\n",
    "            CenterCrop([224, 224]),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "92931a44-de00-48ba-a40e-aeef79ad5669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ds = shuffled['train'].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "18589d00-603f-4324-add3-8c8cfdd7d03f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frost'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = model_ds[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "\n",
    "id2label[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "43f53a60-bc91-4d19-bb4b-9a9655ba83c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 5489\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 1373\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(model_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4d71434e-197c-4a16-8e4c-58f57054b118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = model_ds['train']\n",
    "val_split = model_ds['test'].train_test_split(0.5)\n",
    "val_ds = val_split['train']\n",
    "test_ds = val_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "66db7671-cf84-46be-ad92-f6417d2e4c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)\n",
    "test_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5fae162e-3bcf-41e4-9dc9-1be23e2c830b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b0eed5bd-da5a-49fe-a5a6-21f74ea24356",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "99d96600-25b3-4289-ac81-9782ec0b7bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./weather-base\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=6,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d0d45bf6-c841-40e8-86a7-86c2bb8391a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/felixmorgan/.cache/huggingface/hub/models--microsoft--beit-base-patch16-224-pt22k-ft22k/snapshots/9da301148150e37e533abef672062fa49f6bda4f/config.json\n",
      "Model config BeitConfig {\n",
      "  \"architectures\": [\n",
      "    \"BeitForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auxiliary_channels\": 256,\n",
      "  \"auxiliary_concat_input\": false,\n",
      "  \"auxiliary_loss_weight\": 0.4,\n",
      "  \"auxiliary_num_convs\": 1,\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"dew\",\n",
      "    \"1\": \"fogsmog\",\n",
      "    \"2\": \"frost\",\n",
      "    \"3\": \"glaze\",\n",
      "    \"4\": \"hail\",\n",
      "    \"5\": \"lightning\",\n",
      "    \"6\": \"rain\",\n",
      "    \"7\": \"rainbow\",\n",
      "    \"8\": \"rime\",\n",
      "    \"9\": \"sandstorm\",\n",
      "    \"10\": \"snow\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"dew\": 0,\n",
      "    \"fogsmog\": 1,\n",
      "    \"frost\": 2,\n",
      "    \"glaze\": 3,\n",
      "    \"hail\": 4,\n",
      "    \"lightning\": 5,\n",
      "    \"rain\": 6,\n",
      "    \"rainbow\": 7,\n",
      "    \"rime\": 8,\n",
      "    \"sandstorm\": 9,\n",
      "    \"snow\": 10\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layer_scale_init_value\": 0.1,\n",
      "  \"model_type\": \"beit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"out_indices\": [\n",
      "    3,\n",
      "    5,\n",
      "    7,\n",
      "    11\n",
      "  ],\n",
      "  \"patch_size\": 16,\n",
      "  \"pool_scales\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    6\n",
      "  ],\n",
      "  \"semantic_loss_ignore_index\": 255,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_absolute_position_embeddings\": false,\n",
      "  \"use_auxiliary_head\": true,\n",
      "  \"use_mask_token\": false,\n",
      "  \"use_mean_pooling\": true,\n",
      "  \"use_relative_position_bias\": true,\n",
      "  \"use_shared_relative_position_bias\": false,\n",
      "  \"vocab_size\": 8192\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/felixmorgan/.cache/huggingface/hub/models--microsoft--beit-base-patch16-224-pt22k-ft22k/snapshots/9da301148150e37e533abef672062fa49f6bda4f/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BeitForImageClassification.\n",
      "\n",
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-base-patch16-224-pt22k-ft22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 768]) in the checkpoint and torch.Size([11, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k', num_labels=11,ignore_mismatched_sizes=True, label2id=label2id,\n",
    "    id2label=id2label,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e528e981-2e74-4389-b24f-e20af9709b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felixmorgan/PycharmProjects/pythonProject/./weather-base is already a clone of https://huggingface.co/ChasingMercer/weather-base. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "952f5f00-4f1d-4873-8676-91ec4fece186",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felixmorgan/miniconda3/envs/pythonProject/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5489\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1026\n",
      "  Number of trainable parameters = 85770443\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1026' max='1026' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1026/1026 40:00, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.336800</td>\n",
       "      <td>0.277989</td>\n",
       "      <td>0.900875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.212900</td>\n",
       "      <td>0.233291</td>\n",
       "      <td>0.930029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>0.244026</td>\n",
       "      <td>0.921283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.147500</td>\n",
       "      <td>0.230571</td>\n",
       "      <td>0.931487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>0.219156</td>\n",
       "      <td>0.935860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.218415</td>\n",
       "      <td>0.935860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 686\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./weather-base/checkpoint-171\n",
      "Configuration saved in ./weather-base/checkpoint-171/config.json\n",
      "Model weights saved in ./weather-base/checkpoint-171/pytorch_model.bin\n",
      "Image processor saved in ./weather-base/checkpoint-171/preprocessor_config.json\n",
      "Image processor saved in ./weather-base/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 686\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./weather-base/checkpoint-342\n",
      "Configuration saved in ./weather-base/checkpoint-342/config.json\n",
      "Model weights saved in ./weather-base/checkpoint-342/pytorch_model.bin\n",
      "Image processor saved in ./weather-base/checkpoint-342/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 686\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./weather-base/checkpoint-513\n",
      "Configuration saved in ./weather-base/checkpoint-513/config.json\n",
      "Model weights saved in ./weather-base/checkpoint-513/pytorch_model.bin\n",
      "Image processor saved in ./weather-base/checkpoint-513/preprocessor_config.json\n",
      "Image processor saved in ./weather-base/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 686\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./weather-base/checkpoint-684\n",
      "Configuration saved in ./weather-base/checkpoint-684/config.json\n",
      "Model weights saved in ./weather-base/checkpoint-684/pytorch_model.bin\n",
      "Image processor saved in ./weather-base/checkpoint-684/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 686\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./weather-base/checkpoint-855\n",
      "Configuration saved in ./weather-base/checkpoint-855/config.json\n",
      "Model weights saved in ./weather-base/checkpoint-855/pytorch_model.bin\n",
      "Image processor saved in ./weather-base/checkpoint-855/preprocessor_config.json\n",
      "Image processor saved in ./weather-base/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 686\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./weather-base/checkpoint-1026\n",
      "Configuration saved in ./weather-base/checkpoint-1026/config.json\n",
      "Model weights saved in ./weather-base/checkpoint-1026/pytorch_model.bin\n",
      "Image processor saved in ./weather-base/checkpoint-1026/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./weather-base/checkpoint-855 (score: 0.9358600583090378).\n",
      "Saving model checkpoint to ./weather-base\n",
      "Configuration saved in ./weather-base/config.json\n",
      "Model weights saved in ./weather-base/pytorch_model.bin\n",
      "Image processor saved in ./weather-base/preprocessor_config.json\n",
      "Saving model checkpoint to ./weather-base\n",
      "Configuration saved in ./weather-base/config.json\n",
      "Model weights saved in ./weather-base/pytorch_model.bin\n",
      "Image processor saved in ./weather-base/preprocessor_config.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6d2de99fa342c5968df14bae8a4ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e613a6b01b495b9a44a5527aa2ed2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Mar07_14-11-04_pop-os/events.out.tfevents.1678198277.pop-os.7668.5: 100%|##########| 22.5k/22â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files of refs/heads/main for validity...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/ChasingMercer/weather-base\n",
      "   bcbb22e..aae726f  main -> main\n",
      "\n",
      "To https://huggingface.co/ChasingMercer/weather-base\n",
      "   aae726f..e9efa32  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =          6.0\n",
      "  total_flos               = 2374797442GF\n",
      "  train_loss               =        0.292\n",
      "  train_runtime            =   0:40:02.12\n",
      "  train_samples_per_second =        13.71\n",
      "  train_steps_per_second   =        0.427\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea38ea86-d286-4108-aa96-79ae89f58bbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2402.1234, 'train_samples_per_second': 13.71, 'train_steps_per_second': 0.427, 'total_flos': 2.549919337377528e+18, 'train_loss': 0.2920494159759834, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "print(train_results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9e351ece-f0a7-464d-adcf-48bcc90acbdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file weather-base/checkpoint-855/config.json\n",
      "Model config BeitConfig {\n",
      "  \"_name_or_path\": \"microsoft/beit-base-patch16-224-pt22k-ft22k\",\n",
      "  \"architectures\": [\n",
      "    \"BeitForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auxiliary_channels\": 256,\n",
      "  \"auxiliary_concat_input\": false,\n",
      "  \"auxiliary_loss_weight\": 0.4,\n",
      "  \"auxiliary_num_convs\": 1,\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"dew\",\n",
      "    \"1\": \"fogsmog\",\n",
      "    \"2\": \"frost\",\n",
      "    \"3\": \"glaze\",\n",
      "    \"4\": \"hail\",\n",
      "    \"5\": \"lightning\",\n",
      "    \"6\": \"rain\",\n",
      "    \"7\": \"rainbow\",\n",
      "    \"8\": \"rime\",\n",
      "    \"9\": \"sandstorm\",\n",
      "    \"10\": \"snow\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"dew\": 0,\n",
      "    \"fogsmog\": 1,\n",
      "    \"frost\": 2,\n",
      "    \"glaze\": 3,\n",
      "    \"hail\": 4,\n",
      "    \"lightning\": 5,\n",
      "    \"rain\": 6,\n",
      "    \"rainbow\": 7,\n",
      "    \"rime\": 8,\n",
      "    \"sandstorm\": 9,\n",
      "    \"snow\": 10\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layer_scale_init_value\": 0.1,\n",
      "  \"model_type\": \"beit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"out_indices\": [\n",
      "    3,\n",
      "    5,\n",
      "    7,\n",
      "    11\n",
      "  ],\n",
      "  \"patch_size\": 16,\n",
      "  \"pool_scales\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    6\n",
      "  ],\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"semantic_loss_ignore_index\": 255,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_absolute_position_embeddings\": false,\n",
      "  \"use_auxiliary_head\": true,\n",
      "  \"use_mask_token\": false,\n",
      "  \"use_mean_pooling\": true,\n",
      "  \"use_relative_position_bias\": true,\n",
      "  \"use_shared_relative_position_bias\": false,\n",
      "  \"vocab_size\": 8192\n",
      "}\n",
      "\n",
      "loading weights file weather-base/checkpoint-855/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BeitForImageClassification.\n",
      "\n",
      "All the weights of BeitForImageClassification were initialized from the model checkpoint at weather-base/checkpoint-855.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BeitForImageClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "evluator_model = BeitForImageClassification.from_pretrained('weather-base/checkpoint-855', num_labels=11,ignore_mismatched_sizes=True, label2id=label2id,\n",
    "    id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "76137238-0bd0-4bbf-a44a-a19f056bb739",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felixmorgan/PycharmProjects/pythonProject/./weather-base is already a clone of https://huggingface.co/ChasingMercer/weather-base. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = evaluator_model,\n",
    "    args=training_args,\n",
    "    train_dataset=model_ds[\"train\"],\n",
    "    eval_dataset=model_ds[\"test\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b3015094-8f7c-4735-b80f-76e06fa87252",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [86/86 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13604497909545898, 'eval_accuracy': 0.9534206695778749, 'eval_runtime': 16.1141, 'eval_samples_per_second': 42.634, 'eval_steps_per_second': 5.337}\n"
     ]
    }
   ],
   "source": [
    "evaluator = trainer.evaluate(test_ds)\n",
    "\n",
    "print(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f6d22-6d20-46d0-8215-735d5d5eeb0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
