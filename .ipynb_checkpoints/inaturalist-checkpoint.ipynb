{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee1f475-bf30-4384-bfe5-e93947c166c2",
   "metadata": {
    "id": "lVVgRws2wgT0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import random_split \n",
    "from transformers import ViTFeatureExtractor,  ViTForImageClassification\n",
    "from torch.utils.data import TensorDataset\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e75afcc6-0e7e-47c4-bd31-db5cef23e87f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#Initialise Cuda and check that Cuda is available\n",
    "device = torch.device(\"cuda\")\n",
    "print(device)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfd68b99-2432-4c81-b43a-d5a9d8c06673",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\advanced_topics\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Set model path and create feature_extractor from pretrained Google Model\n",
    "\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84be2cd7-295d-4316-b021-65717b1b61c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2050bada5fc44f2cae4dbd20a40b5973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (C:/Users/felix/.cache/huggingface/datasets/imagefolder/data-042cfdaaf3375786/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6309d2a682fb4894b1f7257726fb4569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load Dataset\n",
    "ds = load_dataset('data/', data_dir='val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7a2d204f-a445-4699-b9df-a6c72c2c18f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Open Json File used to remap annotation label to Class\n",
    "with open('data/val.json') as json_file:\n",
    "    json_data = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5249a67a-64da-4193-ab68-b5f67ffba1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     8\n",
      "1     8\n",
      "2     1\n",
      "3     8\n",
      "4    21\n",
      "5     8\n",
      "6    30\n",
      "7     1\n",
      "8    30\n",
      "9    32\n",
      "dtype: int8\n",
      "['Aves', 'Aves', 'Agaricomycetes', 'Aves', 'Gastropoda', 'Aves', 'Liliopsida', 'Agaricomycetes', 'Liliopsida', 'Magnoliopsida']\n"
     ]
    }
   ],
   "source": [
    "#Iterates through Json File returning the new annotation of class for each index in dataset\n",
    "cat_ids = list([i['category_id'] for i in json_data['annotations'][:]])\n",
    "\n",
    "labels = []\n",
    "\n",
    "\n",
    "for i in cat_ids:\n",
    "    labels.append(json_data['categories'][i]['class'])\n",
    "    \n",
    "# cat_labels = list([json_data['categories'][i]['class'] for i in cats_ids])\n",
    "cats = pd.Series(labels, dtype='category').cat.codes\n",
    "print(cats[0:10])\n",
    "print(labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e4217e99-82ef-4bb2-8eec-8d8bd20857d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove old label and add new label then reshuffle dataset for sampling\n",
    "ds['train'] = ds['train'].remove_columns('labels')\n",
    "ds['train'] = ds['train'].add_column(column=cats, name='labels')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "65b0e00a-9333-4f18-beee-24042091c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(cats.value_counts().loc[lambda x : x>1000].keys())\n",
    "include_index = [i for i, j in enumerate(cats) if j in x]\n",
    "valid_images = ds['train'].select(include_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6d2ea4e4-1fa4-44fe-b17b-d51cfced251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95530\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "include = [j for i, j in enumerate(cats) if j in x]\n",
    "print(len(include))\n",
    "print(len(set(include)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "efc2f124-452e-4a6a-81b9-a024769684d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{32, 1, 34, 2, 0, 4, 8, 42, 45, 21, 26, 30}\n"
     ]
    }
   ],
   "source": [
    "print(set(include))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "93d1e516-d9d9-4786-a283-d3669f3c3d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 1000\n",
      "1 1000\n",
      "21 1000\n",
      "30 1000\n",
      "32 1000\n",
      "34 1000\n",
      "2 1000\n",
      "45 1000\n",
      "26 1000\n",
      "0 1000\n",
      "42 1000\n",
      "4 1000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "def select_indexes(lst, lst_index):\n",
    "    idx_dict = {}\n",
    "    for i, val in enumerate(lst):\n",
    "        if val not in idx_dict:\n",
    "            idx_dict[val] = []\n",
    "        elif len(idx_dict[val]) < 1000:\n",
    "            idx_dict[val].append(lst_index[i])\n",
    "        elif all(len(v) == 1000 for v in idx_dict.values()):\n",
    "            break\n",
    "    return idx_dict\n",
    "\n",
    "sample = []\n",
    "for key, value in select_indexes(include, include_index).items():\n",
    "    print(key, len(value))\n",
    "    sample += value\n",
    "\n",
    "print(len(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "7bcdc1e5-892b-4f03-9c5a-5dbce1813811",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ds = ds['train'].select(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "2660eb6b-7bc6-436e-bb13-391942fdf0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = pd.Series(include)\n",
    "st = s.iloc[sample].astype('category').cat.codes\n",
    "\n",
    "# model_ds = model_ds.remove_columns('labels')\n",
    "model_ds = model_ds.add_column(column=st, name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "1218ca1a-5fa9-4181-9f93-6fdbd621cbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x281 at 0x208EFB7A770>, 'labels': 4}\n"
     ]
    }
   ],
   "source": [
    "print(model_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "1078af05-6177-4cd7-a066-cea9be9360d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "    # Don't forget to include the labels!\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "96020f89-ad7c-4e49-8298-0157c4723dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ds = model_ds.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ba3fc515-82f1-4ff8-abc8-67bad1826fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ds = model_ds.train_test_split(test_size=0.2, shuffle=True)\n",
    "ds_valid = model_ds['test'].train_test_split(test_size=0.5)\n",
    "model_ds['valid'] = ds_valid['test']\n",
    "model_ds['test'] = ds_valid['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da87e5-7fd6-49b6-b603-1351dd78782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ds['test'][0:50]['labels'])\n",
    "train_counter = pd.DataFrame(model_ds['train'])\n",
    "test_counter = pd.DataFrame(model_ds['test'])\n",
    "valid_counter = pd.DataFrame(model_ds['valid'])\n",
    "\n",
    "print(train_counter['labels'].value_counts())\n",
    "print(test_counter['labels'].value_counts())\n",
    "print(valid_counter['labels'].value_counts())\n",
    "\n",
    "\n",
    "# plt.bar(list(set(train_counter['label'])), height=train_counter['label'].value_counts(), log=True)\n",
    "# plt.bar(list(set(test_counter['label'].value_counts())), height=test_counter['label'].value_counts(), log=True)\n",
    "# plt.bar(list(set(valid_counter['label'].value_counts())), height=valid_counter['label'].value_counts(), log=True)\n",
    "\n",
    "# print(max(cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "af816bc8-38cb-4b22-90ea-3b4ca051d06b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "0db7905b-8ef8-45c7-b8e9-7e1b1c865ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "bebd2400-4138-43cd-9034-87f851591288",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\felix/.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k\\snapshots\\7cbdb7ee3a6bcdf99dae654893f66519c480a0f8\\config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\felix/.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k\\snapshots\\7cbdb7ee3a6bcdf99dae654893f66519c480a0f8\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=12\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "cbf2667f-f546-40df-bf13-f258ab0546e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit-base\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=4,\n",
    "  fp16=False,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "6b3bf524-4ce9-4a8c-a697-e7fcac3da449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,  \n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=model_ds[\"train\"],\n",
    "    eval_dataset=model_ds[\"valid\"],\n",
    "    tokenizer=feature_extractor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "517957b7-7d34-4507-992b-e3e76109dac0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\advanced_topics\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9600\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2400\n",
      "  Number of trainable parameters = 85807884\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='2400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  16/2400 06:28 < 18:22:01, 0.04 it/s, Epoch 0.03/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[255], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[0;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39mlog_metrics(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_results\u001b[38;5;241m.\u001b[39mmetrics)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\advanced_topics\\lib\\site-packages\\transformers\\trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1540\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1542\u001b[0m )\n\u001b[1;32m-> 1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\advanced_topics\\lib\\site-packages\\transformers\\trainer.py:1791\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1789\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1791\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1794\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1797\u001b[0m ):\n\u001b[0;32m   1798\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1799\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\advanced_topics\\lib\\site-packages\\transformers\\trainer.py:2557\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2555\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[0;32m   2556\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2557\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\advanced_topics\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\advanced_topics\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24939a-1082-4297-9b03-638c20d19c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator = trainer.evaluate(model_ds['valid'])\n",
    "predictor = trainer.predict(model_ds['test'])\n",
    "\n",
    "predictions = [np.argmax(i) for i in predictor.predictions]\n",
    "print(evaluator)\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "# trainer.log_metrics(\"train\", metrics)\n",
    "# trainer.save_metrics(\"train\", metrics)\n",
    "# trainer.save_metrics(\"eval\", metrics[\"eval_f1\", \"eval_accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89a6e5-9729-49d2-9406-a15cdf0f79d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ds['test'][:10]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d962f689-4b02-40d4-96f0-3491a7021054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
