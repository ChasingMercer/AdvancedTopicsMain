{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e0e343a-87ed-4baf-951e-7ead9bac677a",
   "metadata": {
    "id": "lVVgRws2wgT0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import random_split \n",
    "from transformers import Trainer, TrainingArguments, AutoFeatureExtractor, BeitImageProcessor, BeitForImageClassification\n",
    "from torch.utils.data import TensorDataset\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import evaluate\n",
    "import huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a69ef8a1-9665-4d20-a513-419d632249ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c749ac7decf44c72ab85202ff5c3931b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_hub.notebook_login() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e051ea68-831f-44db-b47c-a21770977729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Initialise Cuda and check that Cuda is available\n",
    "# device = torch.device(\"cuda\")\n",
    "# print(device)\n",
    "# print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e07e20c-3923-4c89-9383-a4b96fd7eeb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felixmorgan/miniconda3/envs/pythonProject/lib/python3.9/site-packages/transformers/models/beit/feature_extraction_beit.py:28: FutureWarning: The class BeitFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use BeitImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "processor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n",
    "model = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ed32d06-37db-475c-bda8-6eb3eb24ac37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'height': 224, 'width': 224}\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n",
    "\n",
    "print(feature_extractor.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0cf5b66-71fe-4dae-8147-74e36e4f5879",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cats_vs_dogs (/home/felixmorgan/.cache/huggingface/datasets/cats_vs_dogs/default/1.0.0/d4fe9cf31b294ed8639aa58f7d8ee13fe189011837038ed9a774fde19a911fcb)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee13c2fdff14dae8beac7212767ddee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load Dataset\n",
    "ds = load_dataset(\"cats_vs_dogs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88b2bbc9-6813-4de6-961e-b18b4bdce46d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=360x268 at 0x7F6DF2C154F0>, 'labels': 0}\n"
     ]
    }
   ],
   "source": [
    "print(ds['train'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9354054a-a570-48b9-830d-51b0fed2654d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop([224, 224]),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize([224, 224]),\n",
    "            CenterCrop([224, 224]),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8dd7dc83-eacd-4a82-9d1f-d89696be080f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/felixmorgan/.cache/huggingface/datasets/cats_vs_dogs/default/1.0.0/d4fe9cf31b294ed8639aa58f7d8ee13fe189011837038ed9a774fde19a911fcb/cache-3f8b93d5734254c1.arrow\n"
     ]
    }
   ],
   "source": [
    "model_ds = ds.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3304974a-a316-4b89-b16f-11698c95ab2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ds = model_ds.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e32b9e89-631d-4c21-97b6-3c87c54746a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ds = model_ds['train'].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4e42fe7d-8847-4339-bd4d-c2e306000ac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = model_ds['train']\n",
    "val_ds = model_ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "34009277-5df3-4b2b-9ba0-001dd10be9af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "692bec58-35fd-4ddc-95a0-b290b918d10a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'labels'],\n",
      "        num_rows: 18728\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'labels'],\n",
      "        num_rows: 4682\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(model_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "af53fbff-fdf9-4ed0-a5cd-58122ea396ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"labels\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f89a81af-a9a9-4736-891b-44fcb666a27f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "095608c2-2d4c-4b98-aba1-75b66db43325",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./beit-base\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1a1230f6-e7c6-4b36-bd8a-14b02fd1ab75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/felixmorgan/.cache/huggingface/hub/models--microsoft--beit-base-patch16-224-pt22k-ft22k/snapshots/9da301148150e37e533abef672062fa49f6bda4f/config.json\n",
      "Model config BeitConfig {\n",
      "  \"architectures\": [\n",
      "    \"BeitForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auxiliary_channels\": 256,\n",
      "  \"auxiliary_concat_input\": false,\n",
      "  \"auxiliary_loss_weight\": 0.4,\n",
      "  \"auxiliary_num_convs\": 1,\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layer_scale_init_value\": 0.1,\n",
      "  \"model_type\": \"beit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"out_indices\": [\n",
      "    3,\n",
      "    5,\n",
      "    7,\n",
      "    11\n",
      "  ],\n",
      "  \"patch_size\": 16,\n",
      "  \"pool_scales\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    6\n",
      "  ],\n",
      "  \"semantic_loss_ignore_index\": 255,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_absolute_position_embeddings\": false,\n",
      "  \"use_auxiliary_head\": true,\n",
      "  \"use_mask_token\": false,\n",
      "  \"use_mean_pooling\": true,\n",
      "  \"use_relative_position_bias\": true,\n",
      "  \"use_shared_relative_position_bias\": false,\n",
      "  \"vocab_size\": 8192\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/felixmorgan/.cache/huggingface/hub/models--microsoft--beit-base-patch16-224-pt22k-ft22k/snapshots/9da301148150e37e533abef672062fa49f6bda4f/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BeitForImageClassification.\n",
      "\n",
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-base-patch16-224-pt22k-ft22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k', num_labels=2,ignore_mismatched_sizes=True)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4ef8f0f0-ecd9-44ff-989a-b64935e82e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felixmorgan/PycharmProjects/pythonProject/./beit-base is already a clone of https://huggingface.co/ChasingMercer/beit-base. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args=training_args,\n",
    "    train_dataset=model_ds[\"train\"],\n",
    "    eval_dataset=model_ds[\"test\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6b8e96ce-d3c2-4e4d-ac04-2b07a4910f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felixmorgan/miniconda3/envs/pythonProject/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 18728\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1755\n",
      "  Number of trainable parameters = 85763522\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1755' max='1755' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1755/1755 1:24:51, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.018584</td>\n",
       "      <td>0.994233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.015015</td>\n",
       "      <td>0.995515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.011637</td>\n",
       "      <td>0.997651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4682\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./beit-base/checkpoint-585\n",
      "Configuration saved in ./beit-base/checkpoint-585/config.json\n",
      "Model weights saved in ./beit-base/checkpoint-585/pytorch_model.bin\n",
      "Image processor saved in ./beit-base/checkpoint-585/preprocessor_config.json\n",
      "Image processor saved in ./beit-base/preprocessor_config.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4682\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./beit-base/checkpoint-1170\n",
      "Configuration saved in ./beit-base/checkpoint-1170/config.json\n",
      "Model weights saved in ./beit-base/checkpoint-1170/pytorch_model.bin\n",
      "Image processor saved in ./beit-base/checkpoint-1170/preprocessor_config.json\n",
      "Image processor saved in ./beit-base/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4682\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./beit-base/checkpoint-1755\n",
      "Configuration saved in ./beit-base/checkpoint-1755/config.json\n",
      "Model weights saved in ./beit-base/checkpoint-1755/pytorch_model.bin\n",
      "Image processor saved in ./beit-base/checkpoint-1755/preprocessor_config.json\n",
      "Image processor saved in ./beit-base/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./beit-base/checkpoint-1755 (score: 0.9976505766766339).\n",
      "Saving model checkpoint to ./beit-base\n",
      "Configuration saved in ./beit-base/config.json\n",
      "Model weights saved in ./beit-base/pytorch_model.bin\n",
      "Image processor saved in ./beit-base/preprocessor_config.json\n",
      "Saving model checkpoint to ./beit-base\n",
      "Configuration saved in ./beit-base/config.json\n",
      "Model weights saved in ./beit-base/pytorch_model.bin\n",
      "Image processor saved in ./beit-base/preprocessor_config.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9515fa42febe4a4f858cccfb4a1ea0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/331M [00:11<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44d61dc1121425f9e16a65b5673b629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Mar06_16-20-32_pop-os/events.out.tfevents.1678119648.pop-os.164622.12:  99%|#########9| 32.0kâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files of refs/heads/main for validity...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/ChasingMercer/beit-base\n",
      "   66bba7d..0d176a1  main -> main\n",
      "\n",
      "To https://huggingface.co/ChasingMercer/beit-base\n",
      "   0d176a1..7e9bfa9  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =          3.0\n",
      "  total_flos               = 4052491304GF\n",
      "  train_loss               =       0.0804\n",
      "  train_runtime            =   1:24:51.75\n",
      "  train_samples_per_second =       11.034\n",
      "  train_steps_per_second   =        0.345\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "57044a9b-582e-4f0e-b123-88852cd298bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(model_ds['train'][45]['pixel_values'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008007c1-bfc4-4d38-858d-2570c5364102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator = trainer.evaluate(model_ds['valid'])\n",
    "predictor = trainer.predict(model_ds['test'])\n",
    "\n",
    "predictions = [np.argmax(i) for i in predictor.predictions]\n",
    "print(evaluator)\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "# trainer.log_metrics(\"train\", metrics)\n",
    "# trainer.save_metrics(\"train\", metrics)\n",
    "# trainer.save_metrics(\"eval\", metrics[\"eval_f1\", \"eval_accuracy\"])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
